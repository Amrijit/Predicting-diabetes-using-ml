{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd077e92358824cd95e6c4bc8bbd85f8f915bed2cbffdf7b71de1dc818ae11a94c8",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\"Diabetes Prediction using Classification Algorithms\"\n",
    "\n",
    "Note: Please Run all the cells at a time and then observe the outputs. Or run the cells serially. please don't run a cell randomly it may show some error. some codes are  commented out because they are needed to extract some informations but not directly connected to the algorithms. And having much lines of code will take a long time to execute. so please remove the comment if you want to see the output of those lines.\n",
    "finally please read the comment of each cell. Thanks\n",
    "                                                        \n",
    "                                                        -Amrijit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "part1: Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "from subprocess import check_output\n",
    "\n",
    "df= pd.read_csv('G:\\ML dataset\\cse445/diabetes_dataset__2019.csv')\n",
    "#print(df.info())\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values:\n",
    "#---------------------\n",
    "\"\"\"\n",
    "df.isnull().any()\n",
    "df.isnull().sum()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null values\n",
    "#---------------------\n",
    "df1= df.dropna()\n",
    "df1= df1.reset_index()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking balance and plotting histogram:\n",
    "#----------------------------------------\n",
    "\"\"\"\n",
    "df1['Diabetic'].value_counts()\n",
    "\n",
    "output=df1['Diabetic']\n",
    "plt.hist(output)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting vaues of categorical Features:\n",
    "#---------------------------------------\n",
    "\"\"\"\n",
    "print(df1.Age.value_counts())\n",
    "print(df1.Gender.value_counts())\n",
    "print(df1.Family_Diabetes.value_counts())\n",
    "print(df1.highBP.value_counts())\n",
    "print(df1.PhysicallyActive.value_counts())\n",
    "print(df1.Smoking.value_counts())\n",
    "print(df1.Alcohol.value_counts())\n",
    "print(df1.RegularMedicine.value_counts())\n",
    "print(df1.JunkFood.value_counts())\n",
    "print(df1.Stress.value_counts())\n",
    "print(df1.BPLevel.value_counts())\n",
    "print(df1.UriationFreq.value_counts())\n",
    "print(df1.Diabetic.value_counts())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique value checking for categorical Features:\n",
    "#------------------------------------------------\n",
    "\"\"\"\n",
    "print(\"unique value in Age: \",df1['Age'].unique())\n",
    "print(\"unique value in gender: \",df1['Gender'].unique())\n",
    "print(\"unique value in Family_Diabetes: \",df['Family_Diabetes'].unique())\n",
    "print(\"unique value in highBP: \",df1['highBP'].unique())\n",
    "print(\"unique value in PhysicallyActive: \",df['PhysicallyActive'].unique())\n",
    "print(\"unique value in Smoking: \",df1['Smoking'].unique())\n",
    "print(\"unique value in Alcohol: \",df1['Alcohol'].unique())\n",
    "print(\"unique value in RegularMedicine: \",df1['RegularMedicine'].unique())\n",
    "print(\"unique value in JunkFood: \",df1['JunkFood'].unique())\n",
    "print(\"unique value in Stress: \",df1['Stress'].unique())\n",
    "print(\"unique value in BPLevel: \",df1['BPLevel'].unique())\n",
    "print(\"unique value in UriationFreq: \",df1['UriationFreq'].unique())\n",
    "print(\"unique value in Diabetic: \",df1['Diabetic'].unique())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting categorical to numerical (label encoding):\n",
    "#------------------------------------------------------\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le= LabelEncoder()\n",
    "df1['N_Age']=le.fit_transform(df1.Age)\n",
    "df1['N_Gender']=le.fit_transform(df1.Gender)\n",
    "df1['N_Family_Diabetes']=le.fit_transform(df1.Family_Diabetes)\n",
    "df1['N_highBP']=le.fit_transform(df1.highBP)\n",
    "df1['N_PhysicallyActive']=le.fit_transform(df1.PhysicallyActive)\n",
    "df1['N_Smoking']=le.fit_transform(df1.Smoking)\n",
    "df1['N_Alcohol']=le.fit_transform(df1.Alcohol)\n",
    "df1['N_RegularMedicine']=le.fit_transform(df1.RegularMedicine)\n",
    "df1['N_JunkFood']=le.fit_transform(df1.JunkFood)\n",
    "df1['N_Stress']=le.fit_transform(df1.Stress)\n",
    "df1['N_BPLevel']=le.fit_transform(df1.BPLevel)\n",
    "df1['N_UriationFreq']=le.fit_transform(df1.UriationFreq)\n",
    "df1['N_Diabetic']=le.fit_transform(df1.Diabetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How it loks after converting-\n",
    "#----------------------------\n",
    "#df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the previous categorical columns: (Don't run this cell multiple times !!!!!)\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "df1= df1.drop(columns= ['index','Age','Gender','Family_Diabetes','highBP','PhysicallyActive','Smoking','Alcohol','RegularMedicine','JunkFood','Stress','BPLevel','UriationFreq','Diabetic'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After dropping the categorical Columns:\n",
    "# -------------------------------------- \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating Features(X) and output/response variable(Y):\n",
    "#-------------------------------------------------------\n",
    "X=df1.drop(columns=['N_Diabetic'])\n",
    "Y=df1['N_Diabetic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection ()\n",
    "#----------------------------------------\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "entropy= mutual_info_classif(X,Y)\n",
    "fit_entrophy= pd.Series(entropy,df.columns[0:len(df1.columns)-1])\n",
    "fit_entrophy.plot(kind='barh', color='teal')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selelction(Featurer Importance{tree based})\n",
    "#----------------------------------------------------\n",
    "\"\"\"\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "importance_model= ExtraTreesClassifier(criterion='gini')\n",
    "importance_model.fit(X,Y)\n",
    "\n",
    "fit_importance= pd.Series(importance_model.feature_importances_, index= X.columns)\n",
    "fit_importance.plot(kind='barh')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection (Univariate selection test (besk k attributes))\n",
    "#------------------------------------------------------------------\n",
    "\"\"\"\"\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "best_features= SelectKBest(score_func= chi2, k=10)\n",
    "fit=best_features.fit(X,Y)\n",
    "\n",
    "feature_scores= pd.DataFrame(fit.scores_)\n",
    "feature_columns= pd.DataFrame(X.columns)\n",
    "\n",
    "concat_feature_metric= pd.concat([feature_columns, feature_scores],  axis=1)\n",
    "concat_feature_metric.columns=['feature', 'Score']\n",
    "\n",
    "print(concat_feature_metric.nlargest(5,'Score'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation (independent-dependent):\n",
    "#--------------------------------------------\n",
    "\"\"\"\n",
    "plt.subplots(figsize=(20,11))\n",
    "heat_plot = sns.heatmap(df1.corr(method='pearson'),annot=True, cmap= \"RdYlGn\")\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation Duplicate cheking (independent-independent):\n",
    "#----------------------------------------------------------------\n",
    "\"\"\"\n",
    "plt.subplots(figsize=(20,11))\n",
    "heat_plot = sns.heatmap(X.corr(method='pearson'),annot=True, cmap= \"RdYlGn\")\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def correlated_features(dataset, threshhold):\n",
    "    column_corr=set()\n",
    "    corr_matrix=dataset.corr()\n",
    "    for i in range (len(corr_matrix.columns)):\n",
    "        for j in range (i):\n",
    "            if(abs(corr_matrix.iloc[i,j])>= threshhold):\n",
    "                add_x= corr_matrix.columns[i]\n",
    "                column_corr.add(add_x)\n",
    "    return column_corr\n",
    "\n",
    "\n",
    "features= correlated_features(X,0.5)\n",
    "print(features)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearman corelation (independent- dependent)\n",
    "#--------------------------------------------\n",
    "\"\"\"\n",
    "plt.subplots(figsize=(20,11))\n",
    "heat_plot = sns.heatmap(df1.corr(method='spearman'),annot=True, cmap= \"RdYlGn\")\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearman correlation (independent-independet)\n",
    "#---------------------------------------------\n",
    "\"\"\"\n",
    "plt.subplots(figsize=(20,11))\n",
    "heat_plot = sns.heatmap(X.corr(method='spearman'),annot=True, cmap= \"RdYlGn\")\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def correlated_features(dataset, threshhold):\n",
    "    column_corr=set()\n",
    "    corr_matrix=dataset.corr()\n",
    "    for i in range (len(corr_matrix.columns)):\n",
    "        for j in range (i):\n",
    "            if(abs(corr_matrix.iloc[i,j])>= threshhold):\n",
    "                add_x= corr_matrix.columns[i]\n",
    "                column_corr.add(add_x)\n",
    "    return column_corr\n",
    "\n",
    "\n",
    "features= correlated_features(X,0.5)\n",
    "print(features)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting best features after using feature selection:\n",
    "#------------------------------------------------------\n",
    "top9=X[[\"N_RegularMedicine\",\"N_BPLevel\",\"N_highBP\",\"N_Age\",\"N_Family_Diabetes\",\"Pregancies\",\"Pdiabetes\",\"N_UriationFreq\",\"N_Stress\"]]\n",
    "top6=X[[\"N_RegularMedicine\",\"N_BPLevel\",\"N_highBP\",\"N_Age\",\"N_Family_Diabetes\",\"Pregancies\"]]\n",
    "top4=X[[\"N_RegularMedicine\",\"N_BPLevel\",\"N_highBP\",\"N_Age\"]]\n"
   ]
  },
  {
   "source": [
    "Note that, here I have used all features while trainning the model. top9, top6, top4 are dataset with highest 9,6,4 features those have highest correlation value with target column. So if you want to use them please select these top9, top6, top4 datasets while trainning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Dataset (standardarization):\n",
    "# ----------------------------------- \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler3= StandardScaler()\n",
    "\n",
    "std_scaled_X= scaler3.fit_transform(X)\n",
    "std_scaled_top9= scaler3.fit_transform(top9)\n",
    "std_scaled_top6= scaler3.fit_transform(top6)\n",
    "std_scaled_top4= scaler3.fit_transform(top4)\n",
    "\n",
    "\"\"\"\n",
    "plt.hist(std_scaled_top4)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Dataset (min-max normalization):\n",
    "# ---------------------------------------\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler1= MinMaxScaler()\n",
    "minmax_scaled_X= scaler1.fit_transform(X)\n",
    "minmax_scaled_top9= scaler1.fit_transform(top9)\n",
    "minmax_scaled_top6= scaler1.fit_transform(top6)\n",
    "minmax_scaled_top4= scaler1.fit_transform(top4)\n",
    "\n",
    "\"\"\"\n",
    "plt.hist(minmax_scaled_top4)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Dataset (Robust normalization) useful on outliars:\n",
    "# ----------------------------------------------------------\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler2= RobustScaler()\n",
    "rob_scaled_x= scaler2.fit_transform(X)\n",
    "rob_scaled_top9= scaler2.fit_transform(top9)\n",
    "rob_scaled_top6= scaler2.fit_transform(top6)\n",
    "rob_scaled_top4= scaler2.fit_transform(top4)\n",
    "\n",
    "plt.hist(rob_scaled_top4)\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "when the distribution is normal then we use normaliation and when the distribution is not normal/ gaussian then we use standardarization. Here I have used all to check accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "part:2\n",
    "Now we are done with data preprocessing. Now its time to use Classification Algorithms\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "#------------------------------------------\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "acc_tree=[]\n",
    "precision_tree=[]\n",
    "recall_tree=[]\n",
    "f1_tree=[]\n",
    "AOC_tree=[]\n",
    "ROC_tree_metrix=[]\n",
    "fpr_tree_metrix=[]\n",
    "tpr_tree_metrix=[]\n",
    "\n",
    "# use commented datasets for spliting for further accuracy checking:-\n",
    "Tree_model= DecisionTreeClassifier(criterion='gini', max_depth= 5)\n",
    "for train_index , test_index in kf.split(minmax_scaled_X):              # std_scaled_X, rob_scaled_x\n",
    "                                                                        # minmax_scaled_top9\n",
    "                                                                        # std_scaled_top9            \n",
    "                                                                        # rob_scaled_top9\n",
    "    X_train, X_test= minmax_scaled_X[train_index], minmax_scaled_X[test_index]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "    Tree_model.fit(X_train,y_train)\n",
    "    Tree_pred_values = Tree_model.predict(X_test)\n",
    "\n",
    "    accuracy_of_tree = accuracy_score(Tree_pred_values , y_test)\n",
    "    acc_tree.append(accuracy_of_tree)\n",
    "    precision_tree.append (precision_score(y_test,Tree_pred_values))\n",
    "    recall_tree.append(recall_score(y_test,Tree_pred_values))\n",
    "    f1_tree.append(f1_score(y_test,Tree_pred_values))\n",
    "    AOC_tree.append(roc_auc_score(y_test,Tree_pred_values))\n",
    "\n",
    "    \n",
    "    tree_roc_y_score= Tree_model.predict_proba(X_test)[:,1]\n",
    "    fpr_tree, tpr_tree, threshold1= roc_curve(y_test,tree_roc_y_score)\n",
    "    fpr_tree_metrix.append(fpr_tree)\n",
    "    tpr_tree_metrix.append(tpr_tree)\n",
    "    \n",
    "\n",
    "\n",
    "print(\"This data is for Decision Tree:-------------------------------------------------\")\n",
    "avg_acc_score_of_tree = sum(acc_tree)/k\n",
    "print(\"Test accuracy of Decision Tree: \",avg_acc_score_of_tree)\n",
    "\n",
    "avg_precision= sum(precision_tree)/k\n",
    "print(\"Precision Score: \",avg_precision)\n",
    "\n",
    "avg_recall_micro= sum(recall_tree)/k\n",
    "print(\"recall Score:\", avg_recall_micro)\n",
    "\n",
    "avg_f1=sum(f1_tree)/k\n",
    "print(\"f1 Score: \", avg_f1)\n",
    "\n",
    "avg_aoc= sum(AOC_tree)/k\n",
    "print(\"AOC score: \",avg_aoc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree ROC Curve:\n",
    "#---------------------------------\n",
    "for i in range(k):\n",
    "    plt.subplots(1, figsize=(10,10))\n",
    "    plt.title('ROC curve for Decision Tree')\n",
    "    plt.plot(fpr_tree_metrix[i],tpr_tree_metrix[i])\n",
    "    plt.plot([0, 1], ls=\"--\")\n",
    "    #plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN classifier:\n",
    "#----------------\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "k= 10\n",
    "kf = KFold(n_splits= k, random_state=None)\n",
    "\n",
    "train_acc_knn=[]\n",
    "test_acc_knn=[]\n",
    "precision_knn=[]\n",
    "recall_knn=[]\n",
    "f1_knn=[]\n",
    "AOC_knn=[]\n",
    "ROC_knn_metrix=[]\n",
    "fpr_knn_metrix=[]\n",
    "tpr_knn_metrix=[]\n",
    "\n",
    "\n",
    "KNNmodel =KNeighborsClassifier(n_neighbors= 4, metric='minkowski', p=2)\n",
    "for train_index , test_index in kf.split(minmax_scaled_X):\n",
    "        X_train, X_test= minmax_scaled_X[train_index], minmax_scaled_X[test_index]\n",
    "        y_train , y_test = Y[train_index] , Y[test_index]\n",
    "        KNNmodel.fit(X_train,y_train)\n",
    "        knn_pred_values = KNNmodel.predict(X_test)\n",
    "\n",
    "        test_accuracy_of_knn = accuracy_score(y_test,knn_pred_values)\n",
    "        test_acc_knn.append(test_accuracy_of_knn)\n",
    "\n",
    "        precision_knn.append (precision_score(y_test,knn_pred_values))\n",
    "        recall_knn.append(recall_score(y_test,knn_pred_values))\n",
    "        f1_knn.append(f1_score(y_test,knn_pred_values))\n",
    "        AOC_knn.append(roc_auc_score(y_test,knn_pred_values))\n",
    "\n",
    "        knn_roc_y_score= KNNmodel.predict_proba(X_test)[:,1]\n",
    "        fpr_knn, tpr_knn, threshold2= roc_curve(y_test,knn_roc_y_score)\n",
    "        fpr_knn_metrix.append(fpr_knn)\n",
    "        tpr_knn_metrix.append(tpr_knn)\n",
    "        \n",
    "\n",
    "print(\"Accuracy of KNN --------------------------------: \")\n",
    "avg_train_acc_knn= sum(train_acc_knn)/k\n",
    "print(\"Train accuracy of KNN: \", avg_train_acc_knn)\n",
    "\n",
    "avg_Test_acc_score_of_knn = sum(test_acc_knn)/k\n",
    "print(\"Test accuracy of KNN: \",avg_Test_acc_score_of_knn)\n",
    "\n",
    "avg_precision_knn= sum(precision_knn)/k\n",
    "print(\"Precision Score: \",avg_precision_knn)\n",
    "\n",
    "avg_recall_knn= sum(recall_knn)/k\n",
    "print(\"recall Score:\", avg_recall_knn)\n",
    "\n",
    "avg_f1_knn= sum(f1_knn)/k\n",
    "print(\"f1 Score: \", avg_f1_knn)\n",
    "\n",
    "avg_aoc_knn= sum(AOC_knn)/k\n",
    "print(\"AOC score: \",avg_aoc_knn)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN ROC Curve:\n",
    "#-----------------------------------------------\n",
    "for i in range(k):\n",
    "    plt.subplots(1, figsize=(10,10))\n",
    "    plt.title('ROC curve for KNN')\n",
    "    plt.plot(fpr_knn_metrix[i],tpr_knn_metrix[i])\n",
    "    plt.plot([0, 1], ls=\"--\")\n",
    "    #plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lorigtic Regression \n",
    "#----------------------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "k= 10\n",
    "kf = KFold(n_splits= k, random_state=None)\n",
    "\n",
    "train_acc_log=[]\n",
    "test_acc_log=[]\n",
    "precision_log=[]\n",
    "recall_log=[]\n",
    "f1_log=[]\n",
    "AOC_log=[]\n",
    "ROC_log_metrix=[]\n",
    "fpr_log_metrix=[]\n",
    "tpr_log_metrix=[]\n",
    "\n",
    "logissticModel = LogisticRegression(random_state = 0)\n",
    "for train_index , test_index in kf.split(minmax_scaled_X):\n",
    "\n",
    "    X_train, X_test= minmax_scaled_X[train_index], minmax_scaled_X[test_index]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "    logissticModel.fit(X_train,y_train)\n",
    "    log_pred_values = logissticModel.predict(X_test)\n",
    "\n",
    "    test_accuracy_of_log = accuracy_score(y_test,log_pred_values)\n",
    "    test_acc_log.append(test_accuracy_of_log)\n",
    "\n",
    "    precision_log.append (precision_score(y_test,log_pred_values))\n",
    "    recall_log.append(recall_score(y_test,log_pred_values))\n",
    "    f1_log.append(f1_score(y_test,log_pred_values))\n",
    "    AOC_log.append(roc_auc_score(y_test,log_pred_values))\n",
    "\n",
    "    log_roc_y_score= logissticModel.predict_proba(X_test)[:,1]\n",
    "    fpr_log, tpr_log, threshold2= roc_curve(y_test,log_roc_y_score)\n",
    "    fpr_log_metrix.append(fpr_log)\n",
    "    tpr_log_metrix.append(tpr_log)\n",
    "\n",
    "print(\"Evaluation of Logistic Regression--------------------------\")\n",
    "avg_train_acc_log= sum(train_acc_log)/k\n",
    "print(\"Train accuracy of KNN: \", avg_train_acc_log)\n",
    "\n",
    "avg_Test_acc_score_of_log = sum(test_acc_log)/k\n",
    "print(\"Test accuracy of KNN: \",avg_Test_acc_score_of_log)\n",
    "\n",
    "avg_precision_log= sum(precision_log)/k\n",
    "print(\"Precision Score: \",avg_precision_log)\n",
    "\n",
    "avg_recall_log= sum(recall_log)/k\n",
    "print(\"recall Score:\", avg_recall_log)\n",
    "\n",
    "avg_f1_log= sum(f1_log)/k\n",
    "print(\"f1 Score: \", avg_f1_log)\n",
    "\n",
    "avg_aoc_log= sum(AOC_log)/k\n",
    "print(\"AOC score: \",avg_aoc_log)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression ROC Curve:\n",
    "#-----------------------------------------------\n",
    "for i in range(k):\n",
    "    plt.subplots(1, figsize=(10,10))\n",
    "    plt.title('ROC curve for Logistic Regression')\n",
    "    plt.plot(fpr_log_metrix[i],tpr_log_metrix[i])\n",
    "    plt.plot([0, 1], ls=\"--\")\n",
    "    #plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppor Vector classifier (SVC) from SVM\n",
    "#--------------------------------------------\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "k= 10\n",
    "kf = KFold(n_splits= k, random_state=None)\n",
    "\n",
    "train_acc_svc=[]\n",
    "test_acc_svc=[]\n",
    "precision_svc=[]\n",
    "recall_svc=[]\n",
    "f1_svc=[]\n",
    "AOC_svc=[]\n",
    "ROC_svc_metrix=[]\n",
    "fpr_svc_metrix=[]\n",
    "tpr_svc_metrix=[]\n",
    "\n",
    "svm= SVC()\n",
    "svcModel= CalibratedClassifierCV(svm)\n",
    "for train_index , test_index in kf.split(minmax_scaled_X):\n",
    "\n",
    "    X_train, X_test= minmax_scaled_X[train_index], minmax_scaled_X[test_index]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "    svcModel.fit(X_train,y_train)\n",
    "    svc_pred_values = svcModel.predict(X_test)\n",
    "\n",
    "    test_accuracy_of_svc = accuracy_score(y_test,svc_pred_values)\n",
    "    test_acc_svc.append(test_accuracy_of_svc)\n",
    "\n",
    "    precision_svc.append (precision_score(y_test,svc_pred_values))\n",
    "    recall_svc.append(recall_score(y_test,svc_pred_values))\n",
    "    f1_svc.append(f1_score(y_test,svc_pred_values))\n",
    "    AOC_svc.append(roc_auc_score(y_test,svc_pred_values))\n",
    "\n",
    "   \n",
    "    #svc_roc_y_score= svcModel.predict_proba(X_test)[:,1]\n",
    "    #fpr_svc, tpr_svc, threshold2= roc_curve(y_test,svc_roc_y_score)\n",
    "    #fpr_svc_metrix.append(fpr_svc)\n",
    "    #tpr_svc_metrix.append(tpr_svc)\n",
    "\n",
    "print(\"Evaluation of SVC--------------------------\")\n",
    "avg_train_acc_svc= sum(train_acc_svc)/k\n",
    "print(\"Train accuracy of KNN: \", avg_train_acc_svc)\n",
    "\n",
    "avg_Test_acc_score_of_svc = sum(test_acc_svc)/k\n",
    "print(\"Test accuracy of KNN: \",avg_Test_acc_score_of_svc)\n",
    "\n",
    "avg_precision_svc= sum(precision_svc)/k\n",
    "print(\"Precision Score: \",avg_precision_svc)\n",
    "\n",
    "avg_recall_svc= sum(recall_svc)/k\n",
    "print(\"recall Score:\", avg_recall_svc)\n",
    "\n",
    "avg_f1_svc= sum(f1_svc)/k\n",
    "print(\"f1 Score: \", avg_f1_svc)\n",
    "\n",
    "avg_aoc_svc= sum(AOC_svc)/k\n",
    "print(\"AOC score: \",avg_aoc_svc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC ROC Curve:\n",
    "#-----------------------------------------------\n",
    "\"\"\"\n",
    "for i in range(k):\n",
    "    plt.subplots(1, figsize=(10,10))\n",
    "    plt.title('ROC curve for Logistic Regression')\n",
    "    plt.plot(fpr_log_metrix[i],tpr_log_metrix[i])\n",
    "    plt.plot([0, 1], ls=\"--\")\n",
    "    #plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "k= 10\n",
    "kf = KFold(n_splits= k, random_state=None)\n",
    "\n",
    "train_acc_forest=[]\n",
    "test_acc_forest=[]\n",
    "precision_forest=[]\n",
    "recall_forest=[]\n",
    "f1_forest=[]\n",
    "AOC_forest=[]\n",
    "ROC_forest_metrix=[]\n",
    "fpr_forest_metrix=[]\n",
    "tpr_forest_metrix=[]\n",
    "\n",
    "forestModel = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "for train_index , test_index in kf.split(minmax_scaled_X):\n",
    "    \n",
    "    X_train, X_test= minmax_scaled_X[train_index], minmax_scaled_X[test_index]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "    forestModel.fit(X_train,y_train)\n",
    "    forest_pred_values = forestModel.predict(X_test)\n",
    "\n",
    "    test_accuracy_of_forest = accuracy_score(y_test,forest_pred_values)\n",
    "    test_acc_forest.append(test_accuracy_of_forest)\n",
    "\n",
    "    precision_forest.append (precision_score(y_test,forest_pred_values))\n",
    "    recall_forest.append(recall_score(y_test,forest_pred_values))\n",
    "    f1_forest.append(f1_score(y_test,forest_pred_values))\n",
    "    AOC_forest.append(roc_auc_score(y_test,forest_pred_values))\n",
    "\n",
    "    forest_roc_y_score= logissticModel.predict_proba(X_test)[:,1]\n",
    "    fpr_forest, tpr_forest, threshold4= roc_curve(y_test,forest_roc_y_score)\n",
    "    fpr_forest_metrix.append(fpr_forest)\n",
    "    tpr_forest_metrix.append(tpr_forest)\n",
    "\n",
    "print(\"Evaluation of Random Forest--------------------------\")\n",
    "avg_train_acc_forest= sum(train_acc_forest)/k\n",
    "print(\"Train accuracy of Random Forest: \", avg_train_acc_forest)\n",
    "\n",
    "avg_Test_acc_score_of_forest = sum(test_acc_forest)/k\n",
    "print(\"Test accuracy of Random Forest: \",avg_Test_acc_score_of_forest)\n",
    "\n",
    "avg_precision_forest= sum(precision_forest)/k\n",
    "print(\"Precision Score: \",avg_precision_forest)\n",
    "\n",
    "avg_recall_forest= sum(recall_forest)/k\n",
    "print(\"recall Score:\", avg_recall_forest)\n",
    "\n",
    "avg_f1_forest= sum(f1_forest)/k\n",
    "print(\"f1 Score: \", avg_f1_forest)\n",
    "\n",
    "avg_aoc_forest= sum(AOC_forest)/k\n",
    "print(\"AOC score: \",avg_aoc_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC ROC Curve:\n",
    "#-----------------------------------------------\n",
    "for i in range(k):\n",
    "    plt.subplots(1, figsize=(10,10))\n",
    "    plt.title('ROC curve for Random Forest')\n",
    "    plt.plot(fpr_forest_metrix[i],tpr_forest_metrix[i])\n",
    "    plt.plot([0, 1], ls=\"--\")\n",
    "    #plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}